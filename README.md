This project was developed as part of the Deep Learning – Fall 2025 course at New York University (NYU Tandon).
It focuses on fine-tuning the Llama-3-8B large language model for the Math Question Answer Verification task from Kaggle, where the objective is to predict whether a student’s solution to a given math problem is correct or incorrect.

Our approach uses Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning and 4-bit quantization through the Unsloth framework to minimize GPU memory usage while maintaining strong performance.


Please downlard the DL_2025_.ipynb to view the code
